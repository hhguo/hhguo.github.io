<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.1.0 for Hugo"><meta name=author content="Haohan Guo"><meta name=description content="Submitted to Arxiv"><link rel=alternate hreflang=en-us href=https://hhguo.github.io/publication/2022_arxiv_lrtts/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.18dd1b2d2475654c7c0c47d29f638859.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hu114e430bfe2fe4b75116576f69263d78_26138_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu114e430bfe2fe4b75116576f69263d78_26138_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://hhguo.github.io/publication/2022_arxiv_lrtts/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Haohan Guo's Page"><meta property="og:url" content="https://hhguo.github.io/publication/2022_arxiv_lrtts/"><meta property="og:title" content="Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations | Haohan Guo's Page"><meta property="og:description" content="Submitted to Arxiv"><meta property="og:image" content="https://hhguo.github.io/publication/2022_arxiv_lrtts/featured.png"><meta property="twitter:image" content="https://hhguo.github.io/publication/2022_arxiv_lrtts/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-11-15T00:00:00+00:00"><meta property="article:modified_time" content="2022-11-15T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://hhguo.github.io/publication/2022_arxiv_lrtts/"},"headline":"Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations","image":["https://hhguo.github.io/publication/2022_arxiv_lrtts/featured.png"],"datePublished":"2022-11-15T00:00:00Z","dateModified":"2022-11-15T00:00:00Z","author":{"@type":"Person","name":"Haohan Guo"},"publisher":{"@type":"Organization","name":"Haohan Guo's Page","logo":{"@type":"ImageObject","url":"https://hhguo.github.io/media/icon_hu114e430bfe2fe4b75116576f69263d78_26138_192x192_fill_lanczos_center_3.png"}},"description":"Submitted to Arxiv"}</script><title>Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations | Haohan Guo's Page</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=f756c2b8e8602066d4c3706abbd5f517><script src=/js/wowchemy-init.min.af22a52d9e52f6033270a248a3e651f7.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Haohan Guo's Page</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Haohan Guo's Page</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/#education><span>Education</span></a></li><li class=nav-item><a class=nav-link href=/uploads/resume.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations</h1><div class=article-metadata><div><span>Haohan Guo</span>, <span>Fenglong Xie</span>, <span>Hui Lu</span>, <span>Xixin Wu</span>, <span>Helen Meng</span></div><span class=article-date>November 2022</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/pdf/2210.15131.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/2022_arxiv_lrtts/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/hhguo/MSMC-TTS target=_blank rel=noopener>Code</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:288px><div style=position:relative><img src=/publication/2022_arxiv_lrtts/featured_huda72ea6bb7ec35d263e13865e9c94c36_395197_720x0_resize_lanczos_3.png alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>This paper aims to enhance low-resource TTS by reducing training data requirements using compact speech representations. A Multi-Stage Multi-Codebook (MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to waveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs from the text for TTS synthesis. Moreover, we optimize the training strategy by leveraging more audio to learn MSMCRs better for low-resource languages. It selects audio from other languages using speaker similarity metric to augment the training set, and applies transfer learning to improve training quality. In MOS tests, the proposed system significantly outperforms FastSpeech and VITS in standard and low-resource scenarios, showing lower data requirements. The proposed training strategy effectively enhances MSMCRs on waveform reconstruction. It improves TTS performance further, which wins 77% votes in the preference test for the low-resource TTS with only 15 minutes of paired data.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">Arxiv</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><div class="alert alert-note"><div>Click the <em>Cite</em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.</div></div></div><div class=article-tags><a class="badge badge-light" href=/tag/msmc-tts/>MSMC-TTS</a>
<a class="badge badge-light" href=/tag/vq-vae/>VQ-VAE</a>
<a class="badge badge-light" href=/tag/low-resource-tts/>Low-Resource TTS</a>
<a class="badge badge-light" href=/tag/vq-gan/>VQ-GAN</a>
<a class="badge badge-light" href=/tag/speech-synthesis/>speech synthesis</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://hhguo.github.io/publication/2022_arxiv_lrtts/&text=Towards%20High-Quality%20Neural%20TTS%20for%20Low-Resource%20Languages%20by%20Learning%20Compact%20Speech%20Representations" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://hhguo.github.io/publication/2022_arxiv_lrtts/&t=Towards%20High-Quality%20Neural%20TTS%20for%20Low-Resource%20Languages%20by%20Learning%20Compact%20Speech%20Representations" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Towards%20High-Quality%20Neural%20TTS%20for%20Low-Resource%20Languages%20by%20Learning%20Compact%20Speech%20Representations&body=https://hhguo.github.io/publication/2022_arxiv_lrtts/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://hhguo.github.io/publication/2022_arxiv_lrtts/&title=Towards%20High-Quality%20Neural%20TTS%20for%20Low-Resource%20Languages%20by%20Learning%20Compact%20Speech%20Representations" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Towards%20High-Quality%20Neural%20TTS%20for%20Low-Resource%20Languages%20by%20Learning%20Compact%20Speech%20Representations%20https://hhguo.github.io/publication/2022_arxiv_lrtts/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://hhguo.github.io/publication/2022_arxiv_lrtts/&title=Towards%20High-Quality%20Neural%20TTS%20for%20Low-Resource%20Languages%20by%20Learning%20Compact%20Speech%20Representations" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/publication/2022_interspeech_msmc/>A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS</a></li><li><a href=/publication/2022_interspeech_msd/>A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS</a></li><li><a href=/publication/2019_interspeech_gantts/>A New GAN-based End-to-End TTS Training Algorithm</a></li><li><a href=/publication/2019_interspeech_parser/>Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS</a></li></ul></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script>
<script src=/en/js/wowchemy.min.5f55938f9a3bd4cdddcfa5326777a1bc.js></script></body></html>
<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.1.0 for Hugo"><meta name=author content="Haohan Guo"><meta name=description content="PhD Student @ CUHK"><link rel=alternate hreflang=en-us href=https://hhguo.github.io/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.18dd1b2d2475654c7c0c47d29f638859.css><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Haohan Guo's Page"><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hu114e430bfe2fe4b75116576f69263d78_26138_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu114e430bfe2fe4b75116576f69263d78_26138_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://hhguo.github.io/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Haohan Guo's Page"><meta property="og:url" content="https://hhguo.github.io/"><meta property="og:title" content="Haohan Guo's Page"><meta property="og:description" content="PhD Student @ CUHK"><meta property="og:image" content="https://hhguo.github.io/media/icon_hu114e430bfe2fe4b75116576f69263d78_26138_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://hhguo.github.io/media/icon_hu114e430bfe2fe4b75116576f69263d78_26138_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2030-06-01T13:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://hhguo.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://hhguo.github.io/"}</script><title>Haohan Guo's Page</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.af22a52d9e52f6033270a248a3e651f7.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Haohan Guo's Page</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Haohan Guo's Page</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured data-target=#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experience data-target=#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/#contact data-target=#contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=/uploads/resume.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" src=/authors/admin/avatar_huc1548f77cf354c3a81fb8d70c959888f_205660_270x270_fill_q75_lanczos_center.jpg alt="Haohan Guo"><div class=portrait-title><h2>Haohan Guo</h2><h3>PhD Student @ CUHK</h3><h3><a href=https://www.cuhk.edu.hk/ target=_blank rel=noopener><span>The Chinese University of Hong Kong</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=mailto:hguo@se.cuhk.edu.hk aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href="https://scholar.google.com.hk/citations?user=B-ZmwcMAAAAJ" target=_blank rel=noopener aria-label=graduation-cap><i class="fas fa-graduation-cap big-icon"></i></a></li><li><a href=https://github.com/hhguo target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=https://www.linkedin.com/in/haohan-guo-9b2216108/ target=_blank rel=noopener aria-label=linkedin><i class="fab fa-linkedin big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Biography</h1><div class=article-style><p>Hello! I am Haohan, a PhD student @ CUHK, supervised by Prof. Helen Mei Ling MENG. Before it, I received my M.S. and B.S. degrees from Northwestern Polytechnical University, supervised by Prof. Lei Xie. Then, I worked as a researcher at Sogou Inc during 2020-2021. My current research topic is deep learning based speech synthesis. If you are interested in my works, welcome to email me.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class=ul-interests><li>Speech & Audio Processing</li><li>Speech Synthesis</li><li>Voice Conversion</li><li>Audio Generation</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>PhD in Computer Science, 2021-</p><p class=institution>The Chinese University of Hong Kong</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>MSc in Computer Science, 2017-2020</p><p class=institution>Northwestern Polytechnical University</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BSc in Computer Science, 2013-2017</p><p class=institution>Northwestern Polytechnical University</p></div></li></ul></div></div></div></div></div></section><section id=featured class="home-section wg-featured"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Publications</h1></div><div class="col-12 col-lg-8"><div class="alert alert-note"><div>Quickly discover relevant content by <a href=./publication/>filtering publications</a>.</div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2022_interspeech_msd/>A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS</a></div><a href=/publication/2022_interspeech_msd/ class=summary-link><div class=article-style>Conference paper accepted by INTERSPEECH 2022</div></a><div class="stream-meta article-metadata"><div><span>Haohan Guo</span>, <span>Hui Lu</span>, <span>Xixin Wu</span>, <span>Helen Meng</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2203.01080.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2022_interspeech_msd/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/2022_interspeech_msd/><img src=/publication/2022_interspeech_msd/featured_hued19287cc0ba410aebae97658f1bcae6_918148_150x0_resize_lanczos_3.png alt="A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS" loading=lazy></a></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2022_interspeech_msmc/>A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS</a></div><a href=/publication/2022_interspeech_msmc/ class=summary-link><div class=article-style>Conference paper accepted by INTERSPEECH 2022</div></a><div class="stream-meta article-metadata"><div><span>Haohan Guo</span>, <span>Fenglong Xie</span>, <span>Frank K. Soong</span>, <span>Xixin Wu</span>, <span>Helen Meng</span></div></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2022_interspeech_msmc/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/2022_interspeech_msmc/><img src=/publication/2022_interspeech_msmc/featured_hu073602e9c296e50a71434d723dfe0066_43404_150x0_resize_lanczos_3.png alt="A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS" loading=lazy></a></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2022_icassp_harsvc/>Improving Adversarial Waveform Generation based Singing Voice Conversion with Harmonic Signals</a></div><a href=/publication/2022_icassp_harsvc/ class=summary-link><div class=article-style>Conference paper accepted by ICASSP 2022</div></a><div class="stream-meta article-metadata"><div><span>Haohan Guo</span>, <span>Zhiping Zhou</span>, <span>Fanbo Meng</span>, <span>Kai Liu</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2201.10130.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2022_icassp_harsvc/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://mycuhk-my.sharepoint.com/:b:/g/personal/1155168203_link_cuhk_edu_hk/Ee3A0EHmMhFEhjN5PuEOpSUBjJKaV5r-31qa1sfMQjZBgQ?e=fnLqiG" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://mycuhk-my.sharepoint.com/:b:/g/personal/1155168203_link_cuhk_edu_hk/EboyOKIG8VFCgxcpuVE8uLABIVlG5o8O7DWBN0HGBwd8wg?e=CqKd1A" target=_blank rel=noopener>Slides</a></div></div><div class=ml-3><a href=/publication/2022_icassp_harsvc/><img src=/publication/2022_icassp_harsvc/featured_hu95e64e5470a4418ded16d27fd6484ea9_230147_150x0_resize_lanczos_3.png alt="Improving Adversarial Waveform Generation based Singing Voice Conversion with Harmonic Signals" loading=lazy></a></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2021_slt_convtts/>Conversational End-to-End TTS for Voice Agents</a></div><a href=/publication/2021_slt_convtts/ class=summary-link><div class=article-style>Conference paper accepted by SLT 2020</div></a><div class="stream-meta article-metadata"><div><span>Haohan Guo</span>, <span>Shaofei Zhang</span>, <span>Frank K. Soong</span>, <span>Lei He</span>, <span>Lei Xie</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2005.10438.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2021_slt_convtts/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/2021_slt_convtts/><img src=/publication/2021_slt_convtts/featured_hua4feb4f0105bad7b9d55c4be5cc70447_41778_150x0_resize_lanczos_3.png alt="Conversational End-to-End TTS for Voice Agents" loading=lazy></a></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2020_arxiv_easvc/>Phonetic Posteriorgrams based Many-to-Many Singing Voice Conversion via Adversarial Training</a></div><a href=/publication/2020_arxiv_easvc/ class=summary-link><div class=article-style>Conference paper submitted to Arxiv</div></a><div class="stream-meta article-metadata"><div><span>Haohan Guo</span>, <span>Heng Lu</span>, <span>Na Hu</span>, <span>Chunlei Zhang</span>, <span>Shan Yang</span>, <span>Lei Xie</span>, <span>Dan Su</span>, <span>Dong Yu</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2012.01837.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2020_arxiv_easvc/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/2020_arxiv_easvc/><img src=/publication/2020_arxiv_easvc/featured_hu6cf1ebdde8c703e98c6b12ea76ef5033_151748_150x0_resize_lanczos_3.png alt="Phonetic Posteriorgrams based Many-to-Many Singing Voice Conversion via Adversarial Training" loading=lazy></a></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2019_interspeech_gantts/>A New GAN-based End-to-End TTS Training Algorithm</a></div><a href=/publication/2019_interspeech_gantts/ class=summary-link><div class=article-style>Conference paper accepted by INTERSPEECH 2019</div></a><div class="stream-meta article-metadata"><div><span>Haohan Guo</span>, <span>Frank K. Soong</span>, <span>Lei He</span>, <span>Lei Xie</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1904.04775.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2019_interspeech_gantts/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/2019_interspeech_gantts/><img src=/publication/2019_interspeech_gantts/featured_hu8068dd595f6b1a633a7d1f08f7dff323_132967_150x0_resize_lanczos_3.png alt="A New GAN-based End-to-End TTS Training Algorithm" loading=lazy></a></div></div><div class="media stream-item"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/2019_interspeech_parser/>Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS</a></div><a href=/publication/2019_interspeech_parser/ class=summary-link><div class=article-style>Conference paper accepted by INTERSPEECH 2019</div></a><div class="stream-meta article-metadata"><div><span>Haohan Guo</span>, <span>Frank K. Soong</span>, <span>Lei He</span>, <span>Lei Xie</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1904.04764.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/2019_interspeech_parser/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/2019_interspeech_parser/><img src=/publication/2019_interspeech_parser/featured_hud0f49474a2636b864da99a0f82250f06_119693_150x0_resize_lanczos_3.png alt="Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS" loading=lazy></a></div></div></div></div></div></section><section id=experience class="home-section wg-experience"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Experience</h1><p class=mt-1>lab, internship, full-time employee</p></div><div class=col-12><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border exp-fill">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="section-subheading card-title exp-title text-muted my-0">PhD student</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.se.cuhk.edu.hk/laboratories/human-computer-communications-laboratory/ target=_blank rel=noopener>HCCL@CUHK</a></div><div class="text-muted exp-meta">Aug 2021 –
Present
<span class=middot-divider></span>
<span>Beijing, China</span></div><div class=card-text>Supervised by Helen Meng.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://ai.sogou.com/ target=_blank rel=noopener><img src=/media/icons/brands/org-sogou.svg width=56px height=56px alt=Sogou></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Researcher</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://ai.sogou.com/ target=_blank rel=noopener>Sogou</a></div><div class="text-muted exp-meta">Dec 2020 –
Jul 2021
<span class=middot-divider></span>
<span>Beijing, China</span></div></div></div><div class=card-text>Work as a researcher on singing voice conversion. We aim to develop a commercial singing conversion system which can convert arbitrary singing voice to the target timbre. High sound quality and accurate melody expression are both required.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://ai.tencent.com/ailab/ target=_blank rel=noopener><img src=/media/icons/brands/org-tencent.svg width=56px height=56px alt="Tencent AI Lab"></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://ai.tencent.com/ailab/ target=_blank rel=noopener>Tencent AI Lab</a></div><div class="text-muted exp-meta">May 2020 –
Dec 2020
<span class=middot-divider></span>
<span>Beijing, China</span></div></div></div><div class=card-text>Research topic is multi-singer singing voice conversion. We propose a MelGAN based end-to-end PPG-SVC model. It significantly improves the sound quality and singer similarity over the conventional PPG-SVC framework. The work is summarized to the paper, <a href=./publication/2020_arxiv_easvc>Phonetic Posteriorgrams based Many-to-Many Singing Voice Conversion via Adversarial Training</a>.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.msra.cn/ target=_blank rel=noopener><img src=/media/icons/brands/org-ms.svg width=56px height=56px alt="Microsoft Research Asia & Microsoft STCA"></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.msra.cn/ target=_blank rel=noopener>Microsoft Research Asia & Microsoft STCA</a></div><div class="text-muted exp-meta">May 2018 –
Sep 2019
<span class=middot-divider></span>
<span>Beijing, China</span></div></div></div><div class=card-text>Supervised by Frank K. Soong and Lei He. We aim to improve the robustness and naturalness of end-to-end TTS. Two main works are published to INTERSPEECH 2019, <a href=./publication/2019_interspeech_gantts>A New GAN-based End-to-End TTS Training Algorithm</a> and <a href=./publication/2019_interspeech_parser>Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS</a>. We also investigate the conversational TTS using the end-to-end approach. The work is published to SLT 2021, <a href=./publication/2021_slt_convtts>Conversational End-to-End TTS for Voice Agents</a>.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.chumenwenwen.com/ target=_blank rel=noopener><img src=/media/icons/brands/org-chumenwenwen.svg width=56px height=56px alt=Chumenwenwen></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.chumenwenwen.com/ target=_blank rel=noopener>Chumenwenwen</a></div><div class="text-muted exp-meta">Sep 2016 –
Jun 2016
<span class=middot-divider></span>
<span>Beijing, China</span></div></div></div><div class=card-text>Be responsible for the optimization of the front-end modules of TTS system, including G2P and Prosody model.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="section-subheading card-title exp-title text-muted my-0">Student</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=http://www.npu-aslp.org/ target=_blank rel=noopener>Audio, Speech and Language Processing Group at Northwestern Polytechnical University (ASLP @ NWPU)</a></div><div class="text-muted exp-meta">Jun 2016 –
May 2020
<span class=middot-divider></span>
<span>Xi'an, Shannxi, China</span></div><div class=card-text>Supervised by Lei Xie.</div></div></div></div></div></div></div></div></section><section id=contact class="home-section wg-contact"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Contact</h1></div><div class="col-12 col-lg-8"><div class=mb-3><form name=contact method=post action=https://formspree.io/f/mgernvwg data-netlify-recaptcha=true><div class="form-group form-inline"><label class=sr-only for=inputName>Name</label>
<input type=text name=name class="form-control w-100" id=inputName placeholder=Name required></div><div class="form-group form-inline"><label class=sr-only for=inputEmail>Email</label>
<input type=email name=email class="form-control w-100" id=inputEmail placeholder=Email required></div><div class=form-group><label class=sr-only for=inputMessage>Message</label>
<textarea name=message class=form-control id=inputMessage rows=5 placeholder=Message required></textarea></div><button type=submit class="btn btn-outline-primary px-3 py-2">Send</button></form></div><ul class=fa-ul><li><i class="fa-li fas fa-envelope fa-2x" aria-hidden=true></i>
<span id=person-email><a href=mailto:hguo@se.cuhk.edu.hk>hguo@se.cuhk.edu.hk</a></span></li><li><i class="fa-li fas fa-map-marker fa-2x" aria-hidden=true></i>
<span id=person-address>William M.W. Mong Engineering Building, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong SAR, China</span></li></ul></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script>
<script src=/en/js/wowchemy.min.5f55938f9a3bd4cdddcfa5326777a1bc.js></script></body></html>